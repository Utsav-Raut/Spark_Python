{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8fd8eb",
   "metadata": {},
   "source": [
    "<h1>Spark:</h1>\n",
    "<ul><li>Spark is a big data tool that allows users to store, process and stream large amounts of data in a quick and fault-tolerant way</li>\n",
    "<li>Spark is written in scala, but offers a Python API to achieve all the tasks using the Python language</li>\n",
    "<li>The building block of scala is called the Resilient Distributed Dataset, or an RDD. All actions and transformations are performed over the RDDs. However do bear in mind that RDDs cannot be modified once they are created. The only way data can be modified is by creating new RDDs mapping from the existing ones</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e4282",
   "metadata": {},
   "source": [
    "<h1>Prerequisites to run PySpark</h1>\n",
    "<ul><li>Apache Spark plus Hadoop</li>\n",
    "<li>Java 8 or higher</li>\n",
    "<li>Python 3</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1cf80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aad3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a727441",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[2]\").setAppName(\"CreatingRDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38febb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b7a36",
   "metadata": {},
   "source": [
    "# 1. Parallelizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd72d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6ab0907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262,\n",
       " ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b5815d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('spark', 1), ('hadoop', 4)], [('spark', 2), ('hadoop', 5)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.collect(), y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031d052",
   "metadata": {},
   "source": [
    "# 2. Create new RDDs from exisitng RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c69f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 15\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "from operator import add\n",
    "adding = nums.reduce(add)\n",
    "print(\"Adding all the elements -> %i\" %(adding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04a66e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2749493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scala',\n",
       " 'java',\n",
       " 'hadoop',\n",
       " 'spark',\n",
       " 'akka',\n",
       " 'spark vs hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark and spark']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sc.parallelize(\n",
    "        [\"scala\",\n",
    "        \"java\",\n",
    "        \"hadoop\",\n",
    "        \"spark\",\n",
    "        \"akka\",\n",
    "        \"spark vs hadoop\",\n",
    "        \"pyspark\",\n",
    "        \"pyspark and spark\"]\n",
    ")\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fed33e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RDD -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(\"Filtered RDD -> %s\" %(filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3645b8",
   "metadata": {},
   "source": [
    "# 3. From external files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7a86376",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('file:///home/boom/Documents/programming/pyspark/data_files/people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc1b2499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Caine, 87', 'Zupper, 22', 'Xerin, 45']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "414ef5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zupper, 22', 'Xerin, 45']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df238781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RDD -> PythonRDD[23] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "dataf = data.flatMap(lambda x: x.split(',')[::2])\n",
    "print(\"Filtered RDD -> %s\" %(dataf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "388335ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael', 'Caine']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016d3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
