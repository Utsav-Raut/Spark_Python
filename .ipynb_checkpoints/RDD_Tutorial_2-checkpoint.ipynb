{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ef9cbe",
   "metadata": {},
   "source": [
    "<p>In-memory computation and parallel processing are the two major reasons why Apache Spark is used in the industry and one of the best frameworks to handle big-data and perform analysis.</p>\n",
    "<p>RDD forms one of the backbones of Apache Spark</p>\n",
    "<p>It is one of the fundamental Data Structures</p>\n",
    "<p>It is a schema-less structure that can handle both structured and unstructured data</p>\n",
    "<p>The data in RDD is split into chunks based on a key and then dispersed across all the executor nodes</p>\n",
    "<p>RDDs are highly resilient, i.e they are able to recover quickly from any issues, as the same data chunks are replicated across multiple executor nodes. Thus even if an executor fails, another will process the data</p>\n",
    "<p>This also allows us to perform functional calculation against our dataset quickly by harnessing the power of multiple nodes. </p>\n",
    "<p>RDDs support two types of operations - Transformations and Actions</p>\n",
    "<p>Transformations are operations applied on a RDD to form a new RDD. The transformations work on the principle of lazy evaluation. Lazy evaluation means that when we call to perform some operations on RDD, it does not execute immediately. Spark maintains the record of which operation is being called through a DAG and since the transformations are lazy in nature we can execute operations any time by calling an action on the data. Hence in lazy evaluation the data is not loaded until necessary. This helps in optimizing the required calculation and recovery of lost data partition</p>\n",
    "<p>Actions are operations performed on an RDD to instruct Spark to apply computations and pass the result back to the driver. The moment an action is invoked all the computations happen which are in the pipeline. This gives us the result that is stored in intermediate storage or distributed file system</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da5fa1",
   "metadata": {},
   "source": [
    "<ul><b>Transformations</b>\n",
    "<li>map</li>\n",
    "<li>flatMap</li>\n",
    "<li>filter</li>\n",
    "<li>distinct</li>\n",
    "<li>reduceByKey</li>\n",
    "<li>mapPartitions</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e036439",
   "metadata": {},
   "source": [
    "<ul><b>Actions</b>\n",
    "<li>collect</li>\n",
    "<li>collectAsMap</li>\n",
    "<li>reduce</li>\n",
    "<li>countByKey</li>\n",
    "<li>take</li>\n",
    "<li>countByValue</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd748e",
   "metadata": {},
   "source": [
    "<h2>Three ways for creating an RDD</h2>\n",
    "<ul>\n",
    "<li>From parallelized collection</li>\n",
    "<li>Another RDD</li>\n",
    "<li>External data sources like HDFS, Amazon S3, Hbase, etc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85aca629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local[2]\").setAppName(\"Sec_RDD_Tut\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f62a6",
   "metadata": {},
   "source": [
    "# Creating RDD from parallelized collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d6be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a RDD from parallelized collection we use the sc.parallelize method\n",
    "# sc stands from SparkContext which can be found under SparkSession.\n",
    "# SparkSession contains SparkContext, StreamingContext and SQLContext\n",
    "# Before Spark 2.0, SparkContext, SQLContext and StreamingContext were distributed separately and had to be loaded separately but now they are put under SparkSession\n",
    "\n",
    "# sc.parallelize is SparkContext's parallelize method for creating a parallelized collection \n",
    "# and it helps Spark to distribute data across multiple nodes instead of depending on a single\n",
    "# node to process the data\n",
    "\n",
    "myRDD = sc.parallelize([('Ross', 19), ('Joey', 18), ('Rachael', 16), ('Phoebe', 18), ('Chandler', 17), ('Monica', 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd807ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ross', 19), ('Joey', 18), ('Rachael', 16)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When an action is invoked, all the computations which are lined in the lineage graph of the \n",
    "# transformations which have been performed on the RDD, take place all at once.\n",
    "# A common approach in spark is to use the collect() method which returns all the values\n",
    "# in the RDD from the Spark worker nodes to the driver node. This can lead to performance\n",
    "# implications when working with large amounts of data as this translates to a large volume\n",
    "# of data being transferred from Spark worker nodes to the driver nodes.\n",
    "# For a small amount of data this is fine, but as a matter of habit, we should always use the\n",
    "# take method.\n",
    "\n",
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fcd8e6",
   "metadata": {},
   "source": [
    "# Creating RDDs from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9756d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_RDD = sc.textFile(\"file:///home/boom/Documents/programming/pyspark/data_files/rd*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca74145b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey there where ya goin’, not exactly knowin’, who says you have to call just one place home. He’s goin’ everywhere, B.J. McKay and his best friend Bear. He just keeps on movin’, ladies keep improvin’, every day is better than the last. New dreams and better scenes, and best of all I don’t pay property tax. Rollin’ down to Dallas, who’s providin’ my palace, off to New Orleans or who knows where. Places new and ladies, too, I’m B.J. McKay and this is my best friend Bear.',\n",
       " '',\n",
       " 'Children of the sun, see your time has just begun, searching for your ways, through adventures every day. Every day and night, with the condor in flight, with all your friends in tow, you search for the Cities of Gold. Ah-ah-ah-ah-ah… wishing for The Cities of Gold. Ah-ah-ah-ah-ah… some day we will find The Cities of Gold. Do-do-do-do ah-ah-ah, do-do-do-do, Cities of Gold. Do-do-do-do, Cities of Gold. Ah-ah-ah-ah-ah… some day we will find The Cities of Gold.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c623518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_RDD = sc.textFile(\"file:////home/boom/Documents/programming/pyspark/my_data.csv\", minPartitions=4).map(lambda element: element.split(\"\\t\"))\n",
    "# map here is being to used to transform a list of string to a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668060da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
