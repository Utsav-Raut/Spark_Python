{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58af35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('RDD_Act1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9746e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
    "inputRDD = spark.sparkContext.parallelize(data)\n",
    "\n",
    "listRDD = spark.sparkContext.parallelize([1,2,3,4,5,3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939ba4a",
   "metadata": {},
   "source": [
    "# aggregate - action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4b121",
   "metadata": {},
   "source": [
    "<p>syntax:</p>\n",
    "\n",
    "<code>\n",
    "def aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)\n",
    "     (implicit arg0: ClassTag[U]): U\n",
    "</code>\n",
    "     \n",
    "\n",
    "Since RDD’s are partitioned, the aggregate takes full advantage of it by first aggregating elements in each partition and then aggregating results of all partition to get the final result. and the result could be any type than the type of your RDD.\n",
    "\n",
    "This takes the following arguments –\n",
    "\n",
    "zeroValue – Initial value to be used for each partition in aggregation, this value would be used to initialize the accumulator. we mostly use 0 for integer and Nil for collections.\n",
    "\n",
    "seqOp – This operator is used to accumulate the results of each partition, and stores the running accumulated result to U,\n",
    "\n",
    "combOp – This operator is used to combine the results of all partitions U."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e665764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "agg = listRDD.aggregate(0, seqOp, combOp)\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef75236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 7)\n"
     ]
    }
   ],
   "source": [
    "seqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "agg2 = listRDD.aggregate((0, 0), seqOp2, combOp2)\n",
    "print(agg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c53cd2",
   "metadata": {},
   "source": [
    "# treeAggregate – action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c21e04",
   "metadata": {},
   "source": [
    "treeAggregate() – Aggregates the elements of this RDD in a multi-level tree pattern. The output of this function will be similar to the aggregate function.\n",
    "\n",
    "Syntax: treeAggregate(zeroValue, seqOp, combOp, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bd61d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "agg = listRDD.treeAggregate(0, seqOp, combOp)\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc1e5a",
   "metadata": {},
   "source": [
    "# fold - action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dd6b6",
   "metadata": {},
   "source": [
    "fold() – Aggregate the elements of each partition, and then the results for all the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a046c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "foldRes = listRDD.fold(0, add)\n",
    "print(foldRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e59d88",
   "metadata": {},
   "source": [
    "# reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebe769",
   "metadata": {},
   "source": [
    "reduce() – Reduces the elements of the dataset using the specified binary operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4670be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "redRes = listRDD.reduce(add)\n",
    "print(redRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32fd7dc",
   "metadata": {},
   "source": [
    "\n",
    "collect() -Return the complete dataset as an Array.\n",
    "\n",
    "count() – Return the count of elements in the dataset.\n",
    "\n",
    "countApprox() – Return approximate count of elements in the dataset, this method returns incomplete when execution time meets timeout.\n",
    "\n",
    "countApproxDistinct() – Return an approximate number of distinct elements in the dataset.\n",
    "\n",
    "countByValue() – Return Map[T,Long] key representing each unique value in dataset and value represents count each value present.\n",
    "\n",
    "\n",
    "first() – Return the first element in the dataset.\n",
    "\n",
    "top() – Return top n elements from the dataset.\n",
    "\n",
    "Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory.\n",
    "    \n",
    "min() – Return the minimum value from the dataset.\n",
    "\n",
    "max() – Return the maximum value from the dataset.\n",
    "\n",
    "take() – Return the first num elements of the dataset.\n",
    "\n",
    "takeOrdered() – Return the first num (smallest) elements from the dataset and this is the opposite of the take() action.\n",
    "Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory.\n",
    "    \n",
    "takeSample() – Return the subset of the dataset in an Array.\n",
    "Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326cad0",
   "metadata": {},
   "source": [
    "# GET THE NUMBER OF PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa27ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0886fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rdd.getNumPartitions()\n",
    "# for dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f447a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
