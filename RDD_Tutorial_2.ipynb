{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ef9cbe",
   "metadata": {},
   "source": [
    "<li>https://www.youtube.com/watch?v=e5ol7oyKV0A&t=633s </li>\n",
    "\n",
    "<p>In-memory computation and parallel processing are the two major reasons why Apache Spark is used in the industry and one of the best frameworks to handle big-data and perform analysis.</p>\n",
    "<p>RDD forms one of the backbones of Apache Spark</p>\n",
    "<p>It is one of the fundamental Data Structures</p>\n",
    "<p>It is a schema-less structure that can handle both structured and unstructured data</p>\n",
    "<p>The data in RDD is split into chunks based on a key and then dispersed across all the executor nodes</p>\n",
    "<p>RDDs are highly resilient, i.e they are able to recover quickly from any issues, as the same data chunks are replicated across multiple executor nodes. Thus even if an executor fails, another will process the data</p>\n",
    "<p>This also allows us to perform functional calculation against our dataset quickly by harnessing the power of multiple nodes. </p>\n",
    "<p>RDDs support two types of operations - Transformations and Actions</p>\n",
    "<p>Transformations are operations applied on a RDD to form a new RDD. The transformations work on the principle of lazy evaluation. Lazy evaluation means that when we call to perform some operations on RDD, it does not execute immediately. Spark maintains the record of which operation is being called through a DAG and since the transformations are lazy in nature we can execute operations any time by calling an action on the data. Hence in lazy evaluation the data is not loaded until necessary. This helps in optimizing the required calculation and recovery of lost data partition</p>\n",
    "<p>Actions are operations performed on an RDD to instruct Spark to apply computations and pass the result back to the driver. The moment an action is invoked all the computations happen which are in the pipeline. This gives us the result that is stored in intermediate storage or distributed file system</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da5fa1",
   "metadata": {},
   "source": [
    "<ul><b>Transformations</b>\n",
    "<li>map</li>\n",
    "<li>flatMap</li>\n",
    "<li>filter</li>\n",
    "<li>distinct</li>\n",
    "<li>reduceByKey</li>\n",
    "<li>mapPartitions</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e036439",
   "metadata": {},
   "source": [
    "<ul><b>Actions</b>\n",
    "<li>collect</li>\n",
    "<li>collectAsMap</li>\n",
    "<li>reduce</li>\n",
    "<li>countByKey</li>\n",
    "<li>take</li>\n",
    "<li>countByValue</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd748e",
   "metadata": {},
   "source": [
    "<h2>Three ways for creating an RDD</h2>\n",
    "<ul>\n",
    "<li>From parallelized collection</li>\n",
    "<li>Another RDD</li>\n",
    "<li>External data sources like HDFS, Amazon S3, Hbase, etc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e71c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local[2]\").setAppName(\"Sec_RDD_Tut\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531285fa",
   "metadata": {},
   "source": [
    "# Creating RDD from parallelized collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3f8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a RDD from parallelized collection we use the sc.parallelize method\n",
    "# sc stands from SparkContext which can be found under SparkSession.\n",
    "# SparkSession contains SparkContext, StreamingContext and SQLContext\n",
    "# Before Spark 2.0, SparkContext, SQLContext and StreamingContext were distributed separately and had to be loaded separately but now they are put under SparkSession\n",
    "\n",
    "# sc.parallelize is SparkContext's parallelize method for creating a parallelized collection \n",
    "# and it helps Spark to distribute data across multiple nodes instead of depending on a single\n",
    "# node to process the data\n",
    "\n",
    "myRDD = sc.parallelize([('Ross', 19), ('Joey', 18), ('Rachael', 16), ('Phoebe', 18), ('Chandler', 17), ('Monica', 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2cf323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ross', 19), ('Joey', 18), ('Rachael', 16)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When an action is invoked, all the computations which are lined in the lineage graph of the \n",
    "# transformations which have been performed on the RDD, take place all at once.\n",
    "# A common approach in spark is to use the collect() method which returns all the values\n",
    "# in the RDD from the Spark worker nodes to the driver node. This can lead to performance\n",
    "# implications when working with large amounts of data as this translates to a large volume\n",
    "# of data being transferred from Spark worker nodes to the driver nodes.\n",
    "# For a small amount of data this is fine, but as a matter of habit, we should always use the\n",
    "# take method.\n",
    "\n",
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d5aeb",
   "metadata": {},
   "source": [
    "# Creating RDDs from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827e8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_RDD = sc.textFile(\"file:///home/boom/Documents/programming/pyspark/data_files/rd*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3571a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey there where ya goin’, not exactly knowin’, who says you have to call just one place home. He’s goin’ everywhere, B.J. McKay and his best friend Bear. He just keeps on movin’, ladies keep improvin’, every day is better than the last. New dreams and better scenes, and best of all I don’t pay property tax. Rollin’ down to Dallas, who’s providin’ my palace, off to New Orleans or who knows where. Places new and ladies, too, I’m B.J. McKay and this is my best friend Bear.',\n",
       " '',\n",
       " 'Children of the sun, see your time has just begun, searching for your ways, through adventures every day. Every day and night, with the condor in flight, with all your friends in tow, you search for the Cities of Gold. Ah-ah-ah-ah-ah… wishing for The Cities of Gold. Ah-ah-ah-ah-ah… some day we will find The Cities of Gold. Do-do-do-do ah-ah-ah, do-do-do-do, Cities of Gold. Do-do-do-do, Cities of Gold. Ah-ah-ah-ah-ah… some day we will find The Cities of Gold.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f49607",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_RDD = sc.textFile(\"file:///home/boom/Documents/programming/pyspark/data_files/my_data.csv\", minPartitions=4).map(lambda element: element.split(\"\\t\"))\n",
    "# map here is being to used to transform a list of string to a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5546a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id,firstname,lastname,email,email2,profession'],\n",
       " ['100,Lynde,Orelee,Lynde.Orelee@yopmail.com,Lynde.Orelee@gmail.com,firefighter'],\n",
       " ['101,Vere,Charity,Vere.Charity@yopmail.com,Vere.Charity@gmail.com,police officer']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d86b2463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_RDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ca0dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_RDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d73f4",
   "metadata": {},
   "source": [
    "# Next we want to perform some actions\n",
    "We want to convert all the data into lowercase and divide the paragraphs into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473ac521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_func(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines\n",
    "split_RDD = new_RDD.map(conv_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be574320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hey',\n",
       "  'there',\n",
       "  'where',\n",
       "  'ya',\n",
       "  'goin’,',\n",
       "  'not',\n",
       "  'exactly',\n",
       "  'knowin’,',\n",
       "  'who',\n",
       "  'says',\n",
       "  'you',\n",
       "  'have',\n",
       "  'to',\n",
       "  'call',\n",
       "  'just',\n",
       "  'one',\n",
       "  'place',\n",
       "  'home.',\n",
       "  'he’s',\n",
       "  'goin’',\n",
       "  'everywhere,',\n",
       "  'b.j.',\n",
       "  'mckay',\n",
       "  'and',\n",
       "  'his',\n",
       "  'best',\n",
       "  'friend',\n",
       "  'bear.',\n",
       "  'he',\n",
       "  'just',\n",
       "  'keeps',\n",
       "  'on',\n",
       "  'movin’,',\n",
       "  'ladies',\n",
       "  'keep',\n",
       "  'improvin’,',\n",
       "  'every',\n",
       "  'day',\n",
       "  'is',\n",
       "  'better',\n",
       "  'than',\n",
       "  'the',\n",
       "  'last.',\n",
       "  'new',\n",
       "  'dreams',\n",
       "  'and',\n",
       "  'better',\n",
       "  'scenes,',\n",
       "  'and',\n",
       "  'best',\n",
       "  'of',\n",
       "  'all',\n",
       "  'i',\n",
       "  'don’t',\n",
       "  'pay',\n",
       "  'property',\n",
       "  'tax.',\n",
       "  'rollin’',\n",
       "  'down',\n",
       "  'to',\n",
       "  'dallas,',\n",
       "  'who’s',\n",
       "  'providin’',\n",
       "  'my',\n",
       "  'palace,',\n",
       "  'off',\n",
       "  'to',\n",
       "  'new',\n",
       "  'orleans',\n",
       "  'or',\n",
       "  'who',\n",
       "  'knows',\n",
       "  'where.',\n",
       "  'places',\n",
       "  'new',\n",
       "  'and',\n",
       "  'ladies,',\n",
       "  'too,',\n",
       "  'i’m',\n",
       "  'b.j.',\n",
       "  'mckay',\n",
       "  'and',\n",
       "  'this',\n",
       "  'is',\n",
       "  'my',\n",
       "  'best',\n",
       "  'friend',\n",
       "  'bear.'],\n",
       " [],\n",
       " ['children',\n",
       "  'of',\n",
       "  'the',\n",
       "  'sun,',\n",
       "  'see',\n",
       "  'your',\n",
       "  'time',\n",
       "  'has',\n",
       "  'just',\n",
       "  'begun,',\n",
       "  'searching',\n",
       "  'for',\n",
       "  'your',\n",
       "  'ways,',\n",
       "  'through',\n",
       "  'adventures',\n",
       "  'every',\n",
       "  'day.',\n",
       "  'every',\n",
       "  'day',\n",
       "  'and',\n",
       "  'night,',\n",
       "  'with',\n",
       "  'the',\n",
       "  'condor',\n",
       "  'in',\n",
       "  'flight,',\n",
       "  'with',\n",
       "  'all',\n",
       "  'your',\n",
       "  'friends',\n",
       "  'in',\n",
       "  'tow,',\n",
       "  'you',\n",
       "  'search',\n",
       "  'for',\n",
       "  'the',\n",
       "  'cities',\n",
       "  'of',\n",
       "  'gold.',\n",
       "  'ah-ah-ah-ah-ah…',\n",
       "  'wishing',\n",
       "  'for',\n",
       "  'the',\n",
       "  'cities',\n",
       "  'of',\n",
       "  'gold.',\n",
       "  'ah-ah-ah-ah-ah…',\n",
       "  'some',\n",
       "  'day',\n",
       "  'we',\n",
       "  'will',\n",
       "  'find',\n",
       "  'the',\n",
       "  'cities',\n",
       "  'of',\n",
       "  'gold.',\n",
       "  'do-do-do-do',\n",
       "  'ah-ah-ah,',\n",
       "  'do-do-do-do,',\n",
       "  'cities',\n",
       "  'of',\n",
       "  'gold.',\n",
       "  'do-do-do-do,',\n",
       "  'cities',\n",
       "  'of',\n",
       "  'gold.',\n",
       "  'ah-ah-ah-ah-ah…',\n",
       "  'some',\n",
       "  'day',\n",
       "  'we',\n",
       "  'will',\n",
       "  'find',\n",
       "  'the',\n",
       "  'cities',\n",
       "  'of',\n",
       "  'gold.'],\n",
       " [],\n",
       " ['top',\n",
       "  'cat!',\n",
       "  'the',\n",
       "  'most',\n",
       "  'effectual',\n",
       "  'top',\n",
       "  'cat!',\n",
       "  'who’s',\n",
       "  'intellectual',\n",
       "  'close',\n",
       "  'friends',\n",
       "  'get',\n",
       "  'to',\n",
       "  'call',\n",
       "  'him',\n",
       "  't.c.,',\n",
       "  'providing',\n",
       "  'it’s',\n",
       "  'with',\n",
       "  'dignity.',\n",
       "  'top',\n",
       "  'cat!',\n",
       "  'the',\n",
       "  'indisputable',\n",
       "  'leader',\n",
       "  'of',\n",
       "  'the',\n",
       "  'gang.',\n",
       "  'he’s',\n",
       "  'the',\n",
       "  'boss,',\n",
       "  'he’s',\n",
       "  'a',\n",
       "  'pip,',\n",
       "  'he’s',\n",
       "  'the',\n",
       "  'championship.',\n",
       "  'he’s',\n",
       "  'the',\n",
       "  'most',\n",
       "  'tip',\n",
       "  'top,',\n",
       "  'top',\n",
       "  'cat.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aac88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_map_RDD = new_RDD.flatMap(conv_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17948979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', 'there', 'where', 'ya']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap flattened out the RDD vertically instead of horizonatally\n",
    "flat_map_RDD.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e7ba7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT WE WANT TO REMOVE ALL THE STOPWORDS FROM THE GIVEN TEXT\n",
    "stopwords = [\"a\", \"all\", \"the\", \"as\", \"is\", \"am\", \"an\", \"and\", \"be\", \"been\", \"from\", \"had\", \"I\", \"I'd\", \"why\", \"with\", \"hey\", \"there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1734bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', 'ya', 'goin’,', 'not']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_RDD = flat_map_RDD.filter(lambda x: x not in stopwords)\n",
    "stop_words_RDD.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969277ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'condor',\n",
       " 'cat!',\n",
       " 'close',\n",
       " 'championship.',\n",
       " 'children',\n",
       " 'cities',\n",
       " 'cat.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "filteredRDD = flat_map_RDD.filter(lambda x: x.startswith('c'))\n",
    "filteredRDD.distinct().take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd3eaf",
   "metadata": {},
   "source": [
    "# Word-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2489a558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 'the'),\n",
       " (9, 'of'),\n",
       " (6, 'and'),\n",
       " (6, 'cities'),\n",
       " (6, 'gold.'),\n",
       " (5, 'he’s'),\n",
       " (4, 'to'),\n",
       " (4, 'day'),\n",
       " (4, 'top'),\n",
       " (3, 'just')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_mapped = flat_map_RDD.map(lambda x: (x,1))\n",
    "rdd_grouped = rdd_mapped.groupByKey()\n",
    "rdd_frequency = rdd_grouped.mapValues(sum).map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "rdd_frequency.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c32064b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_RDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c46b0743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_stop_words_RDD = stop_words_RDD.distinct()\n",
    "distinct_stop_words_RDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b1782fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whe', ['where', 'where.']), ('ya', ['ya']), ('goi', ['goin’,', 'goin’']), ('exa', ['exactly']), ('say', ['says'])]\n"
     ]
    }
   ],
   "source": [
    "similar_starting_words_RDD = distinct_stop_words_RDD.groupBy(lambda w: w[0:3])\n",
    "print([(k, list(v)) for (k,v) in similar_starting_words_RDD.take(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16e07b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_RDD = stop_words_RDD.sample(False, 0.1)\n",
    "# False is the \"with\" replacement parameter. In case I want my output to not have all replacements\n",
    "# so I am assigning False.\n",
    "# .1 is the fraction of data with which we are going to take the sample\n",
    "# 0.1 is 10% of original data, i.e 18 in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c8100b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_RDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffbf4ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'you', 'pay']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8175c86",
   "metadata": {},
   "source": [
    "# Demo of some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f140d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize([('a', 2), ('b', 3)])\n",
    "b = sc.parallelize([('a', 9), ('b', 7), ('c', 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1802744",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.join(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcf86c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (3, 7)), ('a', (2, 9))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38c7af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd = sc.parallelize(range(1, 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dccccda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249975000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31211d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keydata = sc.parallelize([('a', 4), ('b', 3), ('c', 2), ('a', 8), ('d', 2), ('b', 1), ('d', 3)], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0d51e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 3), ('c', 2), ('a', 8), ('d', 2), ('b', 1), ('d', 3)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_keydata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b5a4612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('c', 2), ('a', 12), ('d', 5)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_keydata.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8130625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keydata.saveAsTextFile(\"file:///home/boom/Documents/programming/pyspark/data_files/output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d80c8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "740b30cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(test).sortByKey(True, 1).collect()\n",
    "# True parameter is ascending order of key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d46347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_rdd = sc.parallelize([1, 1, 2, 3])\n",
    "union2 = sc.parallelize([4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58ff8744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_rdd.union(union2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6f6cbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect_rdd_a = sc.parallelize([1, 2, 3, 4])\n",
    "intersect_rdd_b = sc.parallelize([3, 4, 5, 6])\n",
    "intersect_rdd_a.intersection(intersect_rdd_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c8d81f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect_rdd_a.subtract(intersect_rdd_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea72189c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3),\n",
       " (1, 4),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (1, 5),\n",
       " (1, 6),\n",
       " (2, 5),\n",
       " (2, 6),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (3, 5),\n",
       " (3, 6),\n",
       " (4, 5),\n",
       " (4, 6)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect_rdd_a.cartesian(intersect_rdd_b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40591d2d",
   "metadata": {},
   "source": [
    "# Finding Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c364c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page Rank is rank of any page developed by Google. Named after Larry Page of Google.\n",
    "# Higher the page rank, higher is the possibility of it being showed up during search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9dfb37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageLinks = [['a', ['b', 'c', 'd']],\n",
    "            ['c', ['b']], ['b', ['d', 'c']], ['d', ['a', 'c']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7217f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageRanks = [['a', 1], ['c', 1], ['b', 1], ['d', 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "825495b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankContributions(uris, rank):\n",
    "    numberOfUris = len(uris)\n",
    "    rankContribution = float(rank) / numberOfUris\n",
    "    newrank = []\n",
    "    for uri in uris:\n",
    "        newrank.append((uri, rankContribution))\n",
    "    return newrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22f9340b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', ['b', 'c', 'd']], ['c', ['b']], ['b', ['d', 'c']], ['d', ['a', 'c']]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageLinksRDD = sc.parallelize(pageLinks, 2)\n",
    "pageLinksRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e7ba7d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 1], ['c', 1], ['b', 1], ['d', 1]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageRanksRDD = sc.parallelize(pageRanks, 2)\n",
    "pageRanksRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6130e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numIter = 20\n",
    "stamping_factr = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92205f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(numIter):\n",
    "    linksRank = pageLinksRDD.join(pageRanksRDD)\n",
    "    contributedRDD = linksRank.flatMap(lambda x: rankContributions(x[1][0], x[1][1]))\n",
    "    sumRanks = contributedRDD.reduceByKey(lambda v1, v2: v1+v2)\n",
    "    pageRanksRDD = sumRanks.map(lambda x: (x[0], (1-s)+s*x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66bfd85b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 671.0 failed 1 times, most recent failure: Lost task 3.0 in stage 671.0 (TID 149, 192.168.1.102, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-80-c3230108f0b9>\", line 5, in <lambda>\nNameError: name 's' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-80-c3230108f0b9>\", line 5, in <lambda>\nNameError: name 's' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-c807a883030d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpageRanksRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 671.0 failed 1 times, most recent failure: Lost task 3.0 in stage 671.0 (TID 149, 192.168.1.102, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-80-c3230108f0b9>\", line 5, in <lambda>\nNameError: name 's' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/boom/Documents/programming/big_data/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-80-c3230108f0b9>\", line 5, in <lambda>\nNameError: name 's' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n"
     ]
    }
   ],
   "source": [
    "pageRanksRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea783e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
